---
title: "NCAA Project"
author: "Amber Duevel and Sidney Williams"
date: "10/4/2022"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, include = FALSE}
library(tidyverse)
library(dplyr)
library(car)
library(GGally)
```

```{r read in file, message = FALSE, warning = FALSE, include = FALSE}
ace_vb<-read_csv("VB_aces-1.csv")
assists_vb<-read_csv("VB_assists-1.csv")
blocks_vb<-read_csv("VB_blocks-1.csv")
digs_vb<-read_csv("VB_digs-1.csv")
hit_perc_vb<-read_csv("VB_hit_perc-1.csv")
kills_vb<-read_csv("VB_kills-1.csv")
opp_hits_vb<-read_csv("VB_opp_hits-1.csv")
WL_perc_vb<-read_csv("VB_WL_perc-1.csv")

```

```{r join data, include = FALSE}
NCAA_full_set <- ace_vb %>% right_join(assists_vb, by="Team") %>% 
  right_join(blocks_vb, by="Team") %>% 
  right_join(digs_vb, by="Team") %>% 
  right_join(hit_perc_vb, by="Team") %>% 
  right_join(kills_vb, by="Team") %>% 
  right_join(opp_hits_vb, by="Team") %>% 
  right_join(WL_perc_vb, by="Team")
```

```{r clean data, include = FALSE}
names(NCAA_full_set)
NCAA_full_set <- NCAA_full_set %>% select(-c("...1.x", "...1.y", "S.y", "...1.x.x","S.x.x", "...1.y.y", "S.y.y", "...1.x.x.x", "S.x.x.x","...1.y.y.y", "S.y.y.y", "...1.x.x.x.x", "S","...1.y.y.y.y", "Rank.x", "Rank.y", "Rank.x.x", "Rank.y.y", "Rank.x.x.x", "Rank.y.y.y", "Rank.x.x.x.x", "Rank.y.y.y.y", "Kills.y")) 

NCAA_full_set = distinct(NCAA_full_set, Team, .keep_all = TRUE)
```


```{r rename, include = FALSE}
NCAA_full_set <- rename(NCAA_full_set,c("Team_Name"="Team",
                                        "Sets"="S.x",
                                        "Aces"="Aces",
                                        "Per_Set_x"="Per Set.x",
                                        "Assists"="Assists", 
                                        "Per_Set_y"="Per Set.y",
                                        "Block_Solos"="Block Solos",
                                        "Block_Assists"="Block Assists",
                                        "Total_Block"="TB",
                                        "Digs"="Digs",
                                        "Per_Set_xx"="Per Set.x.x",
                                        "Kills"="Kills.x",
                                        "Errors"="Errors", 
                                        "Total_Attacks"="Total Attacks",
                                        "Hitting_Percentage"="Pct..x",
                                        "Percentage_Set"="Per Set.y.y",
                                        "Opponents_Kill"="Opp Kills",
                                        "Opponents_Errors"="Opp Errors",
                                        "Opponents_Attacks"="Opp Attacks",
                                        "Opponents_Percentage"="Opp Pct",
                                        "Win"="W",
                                        "Lost"="L",
                                        "Win_Percentage"="Pct..y"))

```

## Introduction

  In this project we focused on one main research question; what team statistic(s) is most highly predictive of team’s wins and losses. We answered this question by testing for the $R^{2}$ score between win percentages and each team statistic variable. The team statistic variables are things like aces, blocks, assists, etc. This way we can see if any of the variables have a correlation with win percentage. 

## Methods 

   The statistic variables that that were compared to win percentage were ace, assists, block solos, block assists, triple blocks, digs, kills, errors, and total attacks. We focused on three main things when trying to answer our research question using R version 4.2.1. The first one is looking at the summary of simple linear regression model. This told us the $R^{2}$ score for win percentage and each team statistic variable. Next, we looked at the four diagnostic plots. Here, we were able to look to see if there were any outliers and if the data looked normal. Some of the plots looked a little skewed showing non-normality. This is where we used the Box-Cox method. The Box-Cox method showed us if we needed to do any transformations to normalize the data. Before we could conduct the Box-Cox method, we needed to add 0.01 to all the y-values because it doesn’t allow us  to use the method with any zeros. By using the Box-Cox method, we found that none of the data needed to be normalized. 
   
## Results
   
```{r r^2 chart, message = FALSE, warning = FALSE, echo = FALSE}
r_squared_chart<-data.frame(Team_Variable=c("Ace","Assists","Block_Solo","Block_Assists", "Total_Block", "Digs","Kills","Errors","Total_Attacks"), R_2=c(0.07337, 0.1374, 0.004121,0.1491,0.1426,0.008188,0.1384,0.1383, -0.002478), stringsAsFactors = FALSE)
r_squared_chart
```

The table above is showing each team statistic and its corresponding $R^{2}$ value. We found that Block Assists and Total Block had the highest $R^{2}$ values, so we continue to analyze these two team statistics more below. 

```{r ace, include = FALSE}
ace_lm = lm((Win_Percentage+0.01)~Aces, data = NCAA_full_set)
summary(ace_lm)

ace_trans = lm(sqrt(Win_Percentage)~Aces, data = NCAA_full_set)
summary(ace_trans)

ace2_plot = ggplot(aes(x = Aces, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
ace2_plot

par(mfrow = c(2,2))
plot(ace_trans)
```

```{r ace_plot, include = FALSE}
ace_plot = ggplot(aes(x = Aces, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
ace_plot

par(mfrow = c(2,2))
plot(ace_lm)

boxCox(ace_lm)
```


```{r assists, include = FALSE}
assists_lm = lm(Win_Percentage+0.01~Assists, data = NCAA_full_set)
summary(assists_lm)

assists_trans = lm(log(Win_Percentage + 0.01)~Assists, data = NCAA_full_set)
summary(assists_trans)

assists2_plot = ggplot(aes(x = Assists, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
assists2_plot

par(mfrow = c(2,2))
plot(assists_trans)
```

```{r assists_plot, include = FALSE}
assists_plot = ggplot(aes(x = Assists, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
assists_plot

par(mfrow = c(2,2))
plot(assists_lm)

boxCox(assists_lm)
```

```{r block solo, include = FALSE}
block_solo_lm = lm(Win_Percentage+0.01~Block_Solos, data = NCAA_full_set)
summary(block_solo_lm)

block_solo_trans = lm(log(Win_Percentage + 0.01)~Block_Solos, data = NCAA_full_set)
summary(block_solo_trans)

block_solo2_plot = ggplot(aes(x = Block_Solos, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
block_solo2_plot

par(mfrow = c(2,2))
plot(block_solo_trans)
```


```{r block_solo_plot, include = FALSE}
block_solo_plot = ggplot(aes(x = Block_Solos, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
block_solo_plot

par(mfrow = c(2,2))
plot(block_solo_lm)

boxCox(block_solo_lm)
```


```{r block assists, include = FALSE}
block_assists_lm = lm(Win_Percentage+0.01~Block_Assists, data = NCAA_full_set)
summary(block_assists_lm)

block_assists_trans = lm(log(Win_Percentage + 0.01)~Block_Assists, data = NCAA_full_set)
summary(block_assists_trans)

block_assists2_plot = ggplot(aes(x = Block_Assists, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
block_assists2_plot

par(mfrow = c(2,2))
plot(block_assists_trans)
```


```{r block_assists_plot, echo = FALSE, message = FALSE, warning = FALSE}
block_assists_plot = ggplot(aes(x = Block_Assists, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
block_assists_plot

par(mfrow = c(2,2))
plot(block_assists_lm)

boxCox(block_assists_lm)
```
Here we first looked at the linear regression model for the Block Assists variable. The graph shows a moderately strong positive linear correlation. There are a significant amount of points that aren't near the linear line so it's not a great display of the correlation between the block assists and win percentage. The next thing we looked at was the four diagnostic plots for block assists. When looking at the normal qq plot, the points tend to follow the line with a few skewed left at the end and few points skewed right at the beginning. When looking at the residual vs leverage plot there are a few points that are outside of the -2 and 2 range. For the most part a majority of the data points stay in between the two. The plot that was looked at was the Box-Cox plot. This plot showed if we need to add any transformations to normalize our data. We found that it included one which means we don't need any transformations to normalize the data.

```{r total block, include = FALSE}
total_block_lm = lm(Win_Percentage+0.01~Total_Block, data = NCAA_full_set)
summary(total_block_lm)

total_block_trans = lm(log(Win_Percentage + 0.01)~Total_Block, data = NCAA_full_set)
summary(total_block_trans)

total_block2_plot = ggplot(aes(x = Total_Block, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
total_block2_plot

par(mfrow = c(2,2))
plot(total_block_trans)
```


```{r triple_block_plot1, echo = FALSE, message = FALSE, warning = FALSE}
total_block_plot = ggplot(aes(x = Total_Block, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
total_block_plot
```

The scatterplot above shows the relationship between Total Block and Win Percentage. This plot shows a positive relationship where, as a team's total blocks increase, their win percentage will increase.

```{r triple_block_plot2, echo = FALSE, message = FALSE, warning = FALSE}
par(mfrow = c(2,2))
plot(total_block_lm)

boxCox(total_block_lm)
```
In the four diagnostic plots for Total Block, it can be seen in the Normal Q-Q plot that there is skewness on the lower and upper bounds of the plot. This is shown in the points that deviate from the line. In the Scale_Location plot, the red line is roughly horizontal with a slight negative trend in the lower fitted values. In the Residuals vs Leverage plot, it shows a few values with -3 and 3 standardized residuals, but no points in the right corners of the plot which usually suggest possible outliers. 

Looking at the Box-Cox plot, it shows that the best $\lambda$ value is 1, which suggests no transformation to the linear model. 



```{r digs, include = FALSE}
digs_lm = lm(Win_Percentage+0.01~Digs, data = NCAA_full_set)
summary(digs_lm)

digs_trans = lm(log(Win_Percentage + 0.01)~Digs, data = NCAA_full_set)
summary(digs_trans)

digs2_plot = ggplot(aes(x = Digs, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
digs2_plot

par(mfrow = c(2,2))
plot(digs_trans)
```

```{r digs_plot, include = FALSE}
digs_plot = ggplot(aes(x = Digs, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
digs_plot

par(mfrow = c(2,2))
plot(digs_lm)

boxCox(digs_lm)
```


```{r kills, include = FALSE}
kills_lm = lm(Win_Percentage+0.01~Kills, data = NCAA_full_set)
summary(kills_lm)

kills_plot = ggplot(aes(x = Kills, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
kills_plot

par(mfrow = c(2,2))
plot(kills_lm)

boxCox(kills_lm)
# suggests log transformation

kills_trans = lm(log(Win_Percentage + 0.01)~Kills, data = NCAA_full_set)
summary(kills_trans)

kills2_plot = ggplot(aes(x = Kills, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
kills2_plot

par(mfrow = c(2,2))
plot(kills_trans)
```


```{r kills_plot, include = FALSE}
kills_plot = ggplot(aes(x = Kills, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
kills_plot

par(mfrow = c(2,2))
plot(kills_lm)

boxCox(kills_lm)
```


```{r errors, include = FALSE}
errors_lm = lm(Win_Percentage+0.01~Errors, data = NCAA_full_set)
summary(errors_lm)

errors_trans = lm(log(Win_Percentage + 0.01)~Errors, data = NCAA_full_set)
summary(errors_trans)

errors2_plot = ggplot(aes(x = Errors, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
errors2_plot

par(mfrow = c(2,2))
plot(errors_trans)
```

```{r errors_plot, include = FALSE}
errors_plot = ggplot(aes(x = Errors, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
errors_plot

par(mfrow = c(2,2))
plot(errors_lm)

boxCox(errors_lm)
```


```{r total attacks, include = FALSE}
total_attacks_lm = lm(Win_Percentage+0.01~Total_Attacks, data = NCAA_full_set)
summary(total_attacks_lm)

total_attacks_trans = lm(log(Win_Percentage + 0.01)~Total_Attacks, data = NCAA_full_set)
summary(total_attacks_trans)

total_attacks2_plot = ggplot(aes(x = Total_Attacks, y = log(Win_Percentage)), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
total_attacks2_plot

par(mfrow = c(2,2))
plot(total_attacks_trans)
```

```{r total_attacks_plot, include = FALSE}
total_attacks_plot = ggplot(aes(x = Total_Attacks, y = Win_Percentage), data = NCAA_full_set) +
  geom_point() +
  geom_smooth(method = "lm")
total_attacks_plot

par(mfrow = c(2,2))
plot(total_attacks_lm)

boxCox(total_attacks_lm)
```


```{r scatterplot matrix, warning = FALSE, message = FALSE, echo = FALSE}
ggpairs(NCAA_full_set %>% dplyr::select(Win_Percentage, Block_Assists, Total_Block))
```

This scatterplot matrix shows that the three variables, Win Percentage, Block Assists and Total Block, are each approximately normal. This also supports our decision to not transform the data since it is already approximately normal with no noticeable skewness or outliers. 

```{r forest plot, warning = FALSE, message = FALSE, echo = FALSE}
slope_vector = c(coefficients(ace_lm)[2], 
                 coefficients(assists_lm)[2], 
                 coefficients(block_solo_lm)[2], 
                 coefficients(block_assists_lm)[2], 
                 coefficients(total_block_lm)[2], 
                 coefficients(digs_lm)[2], 
                 coefficients(kills_lm)[2], 
                 coefficients(errors_lm)[2], 
                 coefficients(total_attacks_lm)[2])

mod_summary_df = data.frame(mod = c("ace_lm", "assists_lm", "block_solo_lm", "block_assists_lm", "total_block_lm", "digs_lm", "kills_lm", "errors_lm", "total_attacks_lm"), slope_vector) %>% mutate(
  bind_rows(confint(ace_lm)[2,], 
            confint(assists_lm)[2,],
            confint(block_solo_lm)[2,],
            confint(block_assists_lm)[2,],
            confint(total_block_lm)[2,],
            confint(digs_lm)[2,],
            confint(kills_lm)[2,],
            confint(errors_lm)[2,],
            confint(total_attacks_lm)[2,])
)
colnames(mod_summary_df) = c("Model", "Estimate", "Lower_bound", "Upper_bound")

ggplot(data = mod_summary_df, aes(y = Estimate, ymin = Lower_bound, ymax = Upper_bound, x = Model)) +
  geom_pointrange() + 
  geom_hline(yintercept = 0, lty = 2) +
  coord_flip()
```
The forest plot above is showing the confidence intervals for each team statistic variable's slope. If the confidence interval includes 0, that means that there is no correlation between the team statistic and their win percentage. From this plot, we see that the variables Total Attacks, Digs, and Block Solo include 0 in the confidence interval. This also shows that Total Block and Block Assists do not contain 0, so there is a correlation between their team statistics and win percentage. 

After looking at the $R^{2}$ values for each team statistic, as well as the 4 diagnostic plots and scatterplot matrix, we have concluded that Block Assists is the most highly predictive of a team's wins and losses. Between Block Assists and Total Block, Block Assists had a higher $R^{2}$ value, and the 4 diagnostic plots showed more normality with less possible outliers.
